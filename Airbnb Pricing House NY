# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session


os.listdir("/kaggle/input/")
df1 = pd.read_csv('/kaggle/input/new-york-city-airbnb-open-data/AB_NYC_2019.csv', delimiter=',')

df1.head()

valores_null = df1.isnull().sum()
valores_null

# EAD

import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
sns.pairplot(df1)
plt.show()

for feature in df1.columns:
    fig = px.histogram(df1, x = feature, nbins=60, title = feature)
    fig.update_layout(
        autosize=False,
        width=800,
        height=400,)
    fig.show()
    
    
#  CÃ¡lculo de la distancia de Manhattan con las variables Latitud y Longitud

# Average groupby neighbourdhood_group
latitude_m, longitude_m = df1[['latitude', 'longitude']][df1['neighbourhood_group'] == 'Manhattan'].mean()
print('latitude_m: ' + str(latitude_m))
print('longitude_m: ' + str(longitude_m))
df1['distancia'] = abs(df1['latitude'] - latitude_m) + abs(df1['longitude'] - longitude_m)
latitude_m: 40.76506240709098
longitude_m: -73.97460674299501
df1['distancia'] = abs(df1['latitude'] - latitude_m) + abs(df1['longitude'] - longitude_m)
df1.loc[:,['latitude', 'longitude', 'distancia']].head()

latitude	longitude	distancia
0	40.64749	-73.97237	0.119809
1	40.75362	-73.98377	0.020606
2	40.80902	-73.94190	0.076664
3	40.68514	-73.95976	0.094769
4	40.79851	-73.94399	0.064064

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

def one_hot_encoder_10nunique(df, nan_as_category=False):
    original_columns = list(df.columns)
    cat_columns = df.select_dtypes(include=["object","category"]).columns
    cat_columns = [col for col in cat_columns if df[col].nunique() < 10] 
    print("Las columnas a transformar son {}".format(cat_columns))
    df = pd.get_dummies(df, columns=cat_columns,
                        dummy_na=nan_as_category, drop_first=True)
    new_columns = [c for c in df.columns if c not in original_columns]
    return df, new_columns
    
    
label = LabelEncoder()
df13['neighbourhood'] = label.fit_transform(df13['neighbourhood'])

X = df13.drop("price", axis=1)
y = df13["price"]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y,
                                                    test_size=0.2,
                                                    random_state=42)
                                                    

from sklearn.linear_model import LinearRegression, ridge_regression
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import KFold, cross_val_score
# from sklearn.metrics import mean_absolute_percentage_error  # descartado por posibles valores nulos
from sklearn.metrics import mean_squared_error


modelos = []

modelos.append(("LR", LinearRegression()))
#modelos.append(("RI", Ridge(alpha=0.1)))
modelos.append(("DT", DecisionTreeRegressor(random_state=0)))
modelos.append(("KNN",KNeighborsRegressor()))
#modelos.append(("SVM",SVR(kernel="linear")))  # Descartado por tiempo de aprendizaje
modelos.append(("RF", RandomForestRegressor()))
modelos.append(("GB", GradientBoostingRegressor(random_state=0)))


results = []
names = []
scoring= "neg_mean_squared_error"

for name, model in modelos:
    kfold = KFold(n_splits=10, shuffle=True, random_state=42)
    cv_results = 1- cross_val_score(model, X_train, y_train,
                                 scoring=scoring)
    results.append(cv_results)
    names.append(name)
    print("{}: {} +/- {}".format(name, cv_results.mean(),
                                 cv_results.std()))
                                 
 
clf = RandomForestRegressor()
model = RandomizedSearchCV(estimator=clf, param_distributions=param_grid,
                          n_iter=10, cv=4, verbose=4, random_state=42)
model.fit(X_train,y_train)


